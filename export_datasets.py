#!/usr/bin/env python3
"""
Utility script to export and upload datasets from training checkpoints

This script can be used to export datasets from saved training data or
upload existing datasets to Hugging Face.
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict

from huggingface_hub import HfApi, create_repo, upload_folder
from huggingface_hub.utils import HfHubHTTPError

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_dataset_from_jsonl(file_path: str) -> List[Dict]:
    """Load dataset from JSONL file"""
    data = []
    if not os.path.exists(file_path):
        logger.warning(f"File not found: {file_path}")
        return data
    
    with open(file_path, 'r') as f:
        for line in f:
            if line.strip():
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    logger.warning(f"Error parsing line: {e}")
    
    return data


def create_dataset_card(
    train_count: int,
    val_count: int,
    eval_count: int,
    base_model: str = "Qwen/Qwen2.5-7B-Instruct",
    repo_id: str = "mlx-community/qwen-code-rfai-dataset"
) -> str:
    """Create a dataset card"""
    return f"""---
license: mit
task_categories:
- text-generation
language:
- en
tags:
- code-generation
- python
- cpp
- rust
- rfai
- reinforcement-learning
size_categories:
- 1K<n<10K
---

# Qwen Code RFAI Dataset

This dataset contains prompts, teacher-generated code, student-generated code, and scoring parameters from the RFAI (Reinforcement from AI Feedback) training process.

## Dataset Description

This dataset was generated during the fine-tuning of a Qwen model for code generation using RFAI methodology.

## Dataset Structure

- **Training Set**: {train_count} examples
- **Validation Set**: {val_count} examples
- **Evaluation Set**: {eval_count} examples

## Data Fields

Each example contains:
- `prompt` (string): Code generation prompt
- `language` (string): Programming language (python, cpp, rust)
- `student_code` (string): Code generated by student model
- `teacher_code` (string): Reference code from teacher model
- `student_score` (float): Quality score for student code
- `teacher_score` (float): Quality score for teacher code
- `reward` (float): Normalized reward value
- `scoring_breakdown` (dict): Detailed scoring parameters
- `timestamp` (string): ISO timestamp

## Usage

```python
from datasets import load_dataset

dataset = load_dataset("{repo_id}")

# Access training data
train_data = dataset['train']
print(train_data[0])
```

## License

MIT License
"""


def upload_dataset(
    dataset_dir: Path,
    repo_id: str,
    hf_token: str,
    private: bool = False
):
    """Upload dataset to Hugging Face"""
    try:
        # Create repository if it doesn't exist
        try:
            create_repo(
                repo_id=repo_id,
                token=hf_token,
                private=private,
                repo_type="dataset",
                exist_ok=True
            )
            logger.info(f"Dataset repository {repo_id} ready")
        except HfHubHTTPError as e:
            logger.warning(f"Could not create dataset repository: {e}")
            return
        
        # Upload dataset
        logger.info(f"Uploading dataset to {repo_id}...")
        
        upload_folder(
            folder_path=str(dataset_dir),
            repo_id=repo_id,
            token=hf_token,
            commit_message="Upload RFAI training dataset",
            ignore_patterns=["*.pyc", "__pycache__", "*.py"]
        )
        
        logger.info(f"âœ“ Successfully uploaded dataset to https://huggingface.co/datasets/{repo_id}")
        
    except Exception as e:
        logger.error(f"Error uploading dataset: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description="Export and upload datasets to Hugging Face")
    parser.add_argument(
        '--dataset_dir',
        type=str,
        default='./datasets',
        help='Directory containing dataset files (train.jsonl, validation.jsonl, evaluation.jsonl)'
    )
    parser.add_argument(
        '--repo_id',
        type=str,
        required=True,
        help='Hugging Face dataset repository ID (e.g., mlx-community/qwen-code-rfai-dataset)'
    )
    parser.add_argument(
        '--hf_token',
        type=str,
        default=None,
        help='Hugging Face token (or set HUGGINGFACE_TOKEN env var)'
    )
    parser.add_argument(
        '--private',
        action='store_true',
        help='Make repository private'
    )
    parser.add_argument(
        '--base_model',
        type=str,
        default='Qwen/Qwen2.5-7B-Instruct',
        help='Base model name for dataset card'
    )
    
    args = parser.parse_args()
    
    dataset_dir = Path(args.dataset_dir)
    if not dataset_dir.exists():
        logger.error(f"Dataset directory not found: {dataset_dir}")
        return
    
    # Load datasets
    train_file = dataset_dir / "train.jsonl"
    val_file = dataset_dir / "validation.jsonl"
    eval_file = dataset_dir / "evaluation.jsonl"
    
    train_data = load_dataset_from_jsonl(str(train_file)) if train_file.exists() else []
    val_data = load_dataset_from_jsonl(str(val_file)) if val_file.exists() else []
    eval_data = load_dataset_from_jsonl(str(eval_file)) if eval_file.exists() else []
    
    logger.info(f"Loaded datasets:")
    logger.info(f"  Training: {len(train_data)} examples")
    logger.info(f"  Validation: {len(val_data)} examples")
    logger.info(f"  Evaluation: {len(eval_data)} examples")
    
    # Create dataset card
    dataset_card = create_dataset_card(
        train_count=len(train_data),
        val_count=len(val_data),
        eval_count=len(eval_data),
        base_model=args.base_model,
        repo_id=args.repo_id
    )
    
    readme_file = dataset_dir / "README.md"
    with open(readme_file, 'w') as f:
        f.write(dataset_card)
    logger.info(f"Created dataset card at {readme_file}")
    
    # Get HF token
    hf_token = args.hf_token or os.getenv('HUGGINGFACE_TOKEN')
    if not hf_token:
        logger.error("Hugging Face token not provided. Set --hf_token or HUGGINGFACE_TOKEN env var.")
        return
    
    # Upload dataset
    upload_dataset(
        dataset_dir=dataset_dir,
        repo_id=args.repo_id,
        hf_token=hf_token,
        private=args.private
    )
    
    logger.info("Dataset export and upload completed!")


if __name__ == "__main__":
    main()

