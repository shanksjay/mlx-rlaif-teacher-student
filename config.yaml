# Training Configuration for Code Fine-tuning with RLAIF

# Model Configuration
model:
  base_model: "Qwen/Qwen2.5-Coder-3B-Instruct"  # Baseline Qwen model (smaller, faster for M5 MacBook)
  use_4bit: true  # Use 4-bit quantization for M5 MacBook
  use_flash_attention: true
  max_length: 512  # Reduced from 1024 to prevent MPS OOM (can increase if memory allows)
  device_map: "auto"
  use_safetensors: true  # Use safetensors for faster loading
  low_cpu_mem_usage: true  # Optimize memory during loading

# Teacher Model Configuration
teacher:
  provider: "anthropic"  # Options: "openai" or "anthropic"
  model_name: "claude-3-5-haiku-20241022"  # For OpenAI: "gpt-4-turbo-preview", "gpt-3.5-turbo"
  # For Anthropic: Recommended models (will auto-fallback if unavailable):
  #   - "claude-3-5-haiku-20241022" (fastest, cheapest, recommended)
  #   - "claude-3-5-sonnet-20241022" (better quality, slower, more expensive)
  #   - "claude-3-opus-20240229" (deprecated Jan 2026, but may still work)
  #   - "claude-3-sonnet-20240229" (deprecated Jul 2025, but may still work)
  # Note: The system will automatically try fallback models if your requested model is unavailable.
  # Check available models at: https://console.anthropic.com/settings/keys
  api_key_env: "ANTHROPIC_API_KEY"  # or "OPENAI_API_KEY"
  temperature: 0.7
  max_tokens: 512

# Training Configuration
training:
  output_dir: "./checkpoints"
  # Resume from checkpoint (set to checkpoint directory path, e.g., "./checkpoints/checkpoint-e4-end-gs4")
  # Leave as null or omit to start fresh training
  resume_from_checkpoint: null
  num_epochs: 7  # Increased from 3 to 7 for better convergence (5-10 range recommended)
  batch_size: 2  # Reduced from 4 to prevent MPS OOM on 32GB systems
  # Aligned with logging_steps for efficient GPU/memory usage:
  # - gradient_accumulation_steps = 50 matches logging_steps exactly
  # - Optimizer steps happen every 50 batches, aligning with logging boundaries
  # - Effective batch size = 50 * 2 = 100 (slightly larger than previous 64, but better alignment)
  gradient_accumulation_steps: 50  # Aligned with logging_steps (50) for efficient batch boundaries
  learning_rate: 4e-6  # Reduced from 7e-6 to 4e-6 (stabilize training, prevent loss spikes)
  # Optimizer:
  # - adamw: best default on CUDA, but can OOM on Apple Silicon for full fine-tunes due to 2× moment states.
  # - adafactor: much lower optimizer-state memory; recommended on MPS when not using LoRA/QLoRA.
  optimizer: "adamw"
  warmup_steps: 300  # Increased from 200 to 300 (more gradual LR ramp-up for stability with high-variance rewards)
  save_steps: 500
  # Additional checkpointing (independent of optimizer global_step):
  save_every_epochs: 1  # Save a checkpoint at the end of each epoch
  save_every_batches: 0  # Save every N batches (0 = disabled; can be useful for long epochs)
  eval_steps: 250
  logging_steps: 50  # Aligned: matches gradient_accumulation_steps (50) exactly - optimizer steps align with logging boundaries
  max_grad_norm: 0.3  # Reduced from 0.5 to 0.3 (tighter clipping to prevent gradient explosions and loss spikes)
  weight_decay: 0.01
  lr_scheduler_type: "cosine"  # Learning rate scheduling enabled for better convergence
  save_total_limit: 3

# RLAIF Configuration
rlaif:
  reward_weight: 2.0  # Increased from 1.0 to 2.0 (stronger reward signal to improve average reward)
  kl_penalty: 0.15  # Increased from 0.1 to 0.15 (stronger regularization to stabilize loss and prevent model drift)
  reward_threshold: 0.25  # Increased from 0.2 to 0.25 (filter more low-reward samples to reduce noise and improve learning)
  beta: 0.1  # Temperature for reward scaling
  top_k: 50
  top_p: 0.95
  num_samples_per_prompt: 4  # Reduced from 6 to 4 (more focused learning, less exploration noise)
  generation_temperature: 1.0  # Reduced from 1.2 to 1.0 (less exploration, more consistent higher-quality code)
  curriculum_learning: true  # Enable curriculum learning (start easy, increase difficulty)
  reward_bonuses: true  # Enable reward bonuses for specific improvements
  use_lora: true  # Use LoRA for efficient fine-tuning (reduces memory, faster training)
  use_qlora: false  # Use QLoRA (4-bit + LoRA) for maximum efficiency (best for memory-constrained systems)
  lora_r: 16  # LoRA rank (higher = more parameters, better quality but slower)
  lora_alpha: 32  # LoRA alpha (scaling factor, typically 2x rank)
  lora_dropout: 0.05  # LoRA dropout rate
  # lora_target_modules: null  # Auto-detect target modules (or specify: ["q_proj", "v_proj", "k_proj", "o_proj"])

# Data Configuration
data:
  train_file: "./data/train.jsonl"
  eval_file: "./data/eval.jsonl"
  languages: ["python", "cpp", "rust"]
  max_train_samples: null  # null means use all
  max_eval_samples: 100

# Evaluation Configuration
evaluation:
  metrics:
    - "code_quality"
    - "correctness"
    - "readability"
    - "efficiency"
  reward_threshold: 0.7  # Minimum reward score to consider good

# Logging Configuration
logging:
  use_tensorboard: true
  tensorboard_dir: "./logs/tensorboard"
  use_wandb: false
  wandb_project: "code-rlaif"
  log_level: "INFO"
  save_json_summaries: true  # Save batch + epoch summaries to JSONL for offline analysis
  json_summaries_dir: "./logs/json_summaries"  # Writes: batches.jsonl, epochs.jsonl
  baseline_eval_batches: 1  # Compute a pre-training baseline reward on the first N batches (0 = disable)
  tensorboard_batch_interval: 1  # Log per-batch time-series scalars every N batches (1 = every batch)
  # System metrics (System/* in TensorBoard):
  # - monitoring_interval_s: how often we sample in the background thread
  # - system_monitor_step_mode: x-axis for System/* charts ("tick" = sample index, "batch" = aligns with Batch/*)
  monitoring_interval_s: 5
  system_monitor_step_mode: "batch"
  health_check_enabled: true  # Print periodic training health checks while training is running
  health_check_interval_batches: 5  # Run health check every N batches
  health_check_grace_batches: 3  # Suppress reward/baseline warnings for the first N batches of epoch 1 (warmup/noise)
  epoch_health_check_enabled: true  # Enable dynamic parameter adjustment after each epoch based on training trends
  health_check_gen_bottleneck_pct: 85  # Only warn if generation dominates >= this percentage AND tok/s is below target
  health_check_gen_target_tps: 6.0  # Target raw generation tokens/sec (warn if below when gen is dominant)
  # Fragmentation / cache growth health checks (Apple Silicon / Metal):
  # - MPS keeps "driver allocated" memory that can exceed "currently allocated" due to caching/fragmentation.
  # - MLX keeps a Metal cache; growth can indicate fragmentation or memory pressure.
  health_check_fragmentation_enabled: true
  # NOTE: On Apple Silicon, this value can be high but stable due to caching.
  # We trigger GC primarily on *growth*; keep this as a “high watermark” only.
  health_check_mps_fragmentation_gb: 10.0  # High watermark for (MPS_driver_alloc - MPS_current_alloc)
  health_check_mlx_cache_gb: 3.0           # Warn/GC if MLX Metal cache exceeds this
  health_check_fragmentation_growth_gb: 0.75  # Warn/GC if fragmentation grows by >= this since last check
  health_check_trigger_gc_on_fragmentation: true
  health_check_gc_cooldown_batches: 10     # Don't trigger GC more often than this many batches
  # GPU utilization on Apple Silicon:
  # - "memory_proxy": uses MPS memory usage as a crude proxy (fast, but can disagree with Activity Monitor, especially when using MLX)
  # - "powermetrics": uses macOS `powermetrics` GPU sampler when available (more accurate, may require elevated permissions)
  gpu_utilization_mode: "memory_proxy"

# Hardware Configuration (M5 MacBook)
hardware:
  use_mps: true  # Use Metal Performance Shaders for M5
  mixed_precision: "bf16"  # bfloat16 for M5
  dataloader_num_workers: 4
  # Experimental: warm the MPS allocator by allocating+freeing some large chunks at startup.
  # Can reduce early fragmentation spikes on some setups; set to 0 to disable.
  mps_allocator_warmup_gb: 0.0
  save_mlx_format: true  # LoRA-aware MLX export will merge adapters before convert
  mlx_quantization: q8_bit  # Options: "q4_bit", "q8_bit", or null for no quantization
  use_mlx_for_generation: true
  require_mlx_for_generation: true
  allow_4bit_on_mps: false  # If true, allow BitsAndBytes 4-bit on MPS (NOT recommended; can cause NaNs)
  reload_mlx_from_latest_checkpoint: false  # LoRA uses a dedicated MLX-sync pipeline (below)
  # MLX Metal cache limit (helps reduce long-run fragmentation). 0 = unlimited (default).
  mlx_metal_cache_limit_gb: 1.0
  # MLX generation worker process (recommended when mixing MLX generation + PyTorch MPS training):
  # Runs MLX generation in a separate process to reduce Metal allocator contention/fragmentation.
  use_mlx_generation_worker: true
  # LoRA + MLX generation:
  # We periodically (after optimizer steps) export a merged HF model (base + adapters),
  # run mlx_lm.convert, then hot-swap the MLX generation worker to the new weights.
  lora_mlx_sync_enabled: true
  lora_mlx_sync_every_optimizer_steps: 1
  mlx_generation_worker_timeout_s: 240
  # Auto-detects MLX model if mlx_model_path is null by checking common locations
  mlx_model_path: ./mlx_model/q4  # Path to MLX model (e.g., "./mlx_model/q8", "./mlx_model/q4", "./mlx_model/base")
                         # If null, will auto-detect from: ./mlx_model/q8, ./mlx_model/q4, ./mlx_model/base

  # NVIDIA / CUDA Configuration (for comparison runs on a Linux/Windows box w/ NVIDIA GPU)
  # Unsloth only works on CUDA. On Apple Silicon it will be ignored automatically.
  use_unsloth: false  # Enable Unsloth-optimized loading/training/generation on CUDA
  unsloth_dtype: "bf16"  # "bf16" (recommended on newer GPUs) or "fp16"
  unsloth_max_seq_length: null  # If null, uses model.max_length

# Hugging Face Upload Configuration
huggingface:
  upload_to_hub: false  # Set to true to upload model to Hugging Face
  repo_id: "mlx-community/code-rlaif"  # Format: mlx-community/model-name
  hf_token_env: "HUGGINGFACE_TOKEN"  # Environment variable with HF token
  upload_quantized: true  # Upload quantized version (4-bit) to HF
  private: false  # Make repository private
  upload_datasets: true  # Upload training/validation/evaluation datasets
  dataset_repo_id: "mlx-community/code-rlaif-dataset"  # Dataset repository ID
  save_datasets_locally: true  # Save datasets locally before upload
  dataset_output_dir: "./datasets"  # Local directory to save datasets

