model:
  base_model: Qwen/Qwen2.5-Coder-3B-Instruct
  use_4bit: true
  use_flash_attention: true
  max_length: 512
  device_map: auto
  use_safetensors: true
  low_cpu_mem_usage: true
teacher:
  provider: anthropic
  model_name: claude-3-5-haiku-20241022
  api_key_env: ANTHROPIC_API_KEY
  temperature: 0.0  # Temperature for scoring (generation uses this value but scoring is hardcoded to 0.0)
  max_tokens: 512
  teacher_score_cache_max_age_seconds: 14400  # Cache TTL: 4 hours (14400s) to reduce cache misses during baseline + early epochs. Increase to 28800 (8h) or 86400 (24h) if needed.
training:
  output_dir: ./checkpoints
  resume_from_checkpoint: null
  num_epochs: 7
  batch_size: 6  # Increased from 4 to reduce gradient noise (more prompts per batch)
  gradient_accumulation_steps: 3   # Increased from 2 to reduce gradient noise (effective batch = 18 prompts per optimizer step)
  generation_accumulation_batches: 1
  learning_rate: 1.5e-5            # Reduced from 3.0e-5 to stabilize training and reduce loss jumps
  optimizer: adamw
  warmup_steps: 8  # Set to 5-10 (or ~0.05-0.1 of total steps) for LoRA
  save_steps: 500
  save_every_epochs: 1
  save_every_batches: 0
  eval_steps: 250
  logging_steps: 3                 # Aligned with gradient_accumulation_steps: 3 (logs every optimizer step)
  max_grad_norm: 1.0
  weight_decay: 0.0
  lr_scheduler_type: cosine
  save_total_limit: 3
rlaif:
  reward_weight: 1.2
  kl_penalty: 0.10
  adaptive_kl_enabled: true  # Enable adaptive KL controller
  target_kl: 0.075  # Target KL divergence (0.05-0.10 for code tasks)
  kl_gain: 0.1  # Gain factor for adaptive KL controller (k in exp(k*(observed_kl - target_kl)))
  reward_threshold: 0.02  # Filter low-quality samples (was 0.0)
  # KL penalty relaxation based on reward (allows exploration when rewards are low)
  use_reward_based_kl_relaxation: true  # Enable KL penalty relaxation when reward is low
  kl_relaxation_reward_threshold: 0.5  # If avg_reward < this, reduce KL penalty to allow exploration
  kl_relaxation_decay_factor: 0.95  # Multiply kl_penalty by this factor when reward is low
  kl_relaxation_min_penalty: 0.01  # Minimum KL penalty (floor to prevent complete removal)
  beta: 0.1
  top_k: 50
  top_p: 0.9  # Reduced from 0.95 to reduce variance and raise mean reward
  num_samples_per_prompt: 4  # Increased from 2 to 4 for better per-prompt baselines and reduced gradient noise
  top_samples_per_prompt: 1  # Train on top-1 sample per prompt (better baseline from 4 samples improves signal-to-noise)
  generation_temperature: 0.8  # Increased to 0.8 for maximum exploration diversity to find higher reward paths
  curriculum_learning: true
  reward_bonuses: false
  use_advantage_normalization: true
  advantage_baseline_type: per_prompt  # Use per-prompt group baseline (mean reward across N samples for the prompt)
  advantage_baseline_ema_alpha: 0.8
  reward_temperature: 0.1  # Temperature for reward stretching (lower = more emphasis on best response)
  use_reward_stretching: true  # Use temperature-scaled softmax to stretch reward signal for better gradient signal
  use_frozen_reference_for_kl: true
  use_tiered_scoring: true
  heuristic_score_threshold: 0.3
  truncate_prompt_for_scoring: true
  prompt_context_chars: 200
  move_rubric_to_system_prompt: true
  use_lora: true
  use_qlora: false
  lora_r: 32  # Increased from 16 for more "logical" capacity
  lora_alpha: 64  # Maintain 2x rank ratio (2 * 32)
  lora_dropout: 0.05
  use_rslora: true  # Use Rank-Stabilized LoRA for faster convergence
data:
  train_file: ./data/train.jsonl
  eval_file: ./data/eval.jsonl
  languages:
  - python
  - cpp
  - rust
  max_train_samples: null
  max_eval_samples: 100
evaluation:
  metrics:
  - code_quality
  - correctness
  - readability
  - efficiency
  reward_threshold: 0.7
logging:
  use_tensorboard: true
  tensorboard_dir: ./logs/tensorboard
  use_wandb: false
  wandb_project: code-rlaif
  log_level: INFO
  save_json_summaries: true
  json_summaries_dir: ./logs/json_summaries
  baseline_eval_batches: 10  # 10 batches × 24 samples/batch (batch_size=6 × num_samples_per_prompt=4) = 240 completions (above 80-160 target for stable reference)
  tensorboard_batch_interval: 1
  monitoring_interval_s: 5
  system_monitor_step_mode: batch
  health_check_enabled: true
  health_check_interval_batches: 5
  health_check_grace_batches: 3
  epoch_health_check_enabled: true
  within_epoch_trend_detection_enabled: false
  health_check_gen_bottleneck_pct: 85
  health_check_gen_target_tps: 6.0
  health_check_fragmentation_enabled: true
  health_check_mps_fragmentation_gb: 10.0
  health_check_mlx_cache_gb: 3.0
  health_check_fragmentation_growth_gb: 0.75
  health_check_trigger_gc_on_fragmentation: true
  health_check_gc_cooldown_batches: 10
  gpu_utilization_mode: memory_proxy
hardware:
  use_mps: true
  mixed_precision: bf16
  dataloader_num_workers: 4
  mps_allocator_warmup_gb: 0.0
  save_mlx_format: true
  mlx_quantization: q4_bit  # Match the deployed MLX model (q4) for speed - was q8_bit which caused mismatch
  use_mlx_for_generation: true
  require_mlx_for_generation: true
  allow_4bit_on_mps: false
  reload_mlx_from_latest_checkpoint: false
  mlx_metal_cache_limit_gb: 1.0
  use_mlx_generation_worker: true
  lora_mlx_sync_enabled: true
  lora_mlx_sync_every_optimizer_steps: 10  # Sync every 8-10 optimizer steps (reduces overhead significantly)
  mlx_generation_worker_timeout_s: 240
  mlx_model_path: ./mlx_model/q4
  use_unsloth: false
  unsloth_dtype: bf16
  unsloth_max_seq_length: null
huggingface:
  upload_to_hub: false
  repo_id: mlx-community/code-rlaif
  hf_token_env: HUGGINGFACE_TOKEN
  upload_quantized: true
  private: false
  upload_datasets: true
  dataset_repo_id: mlx-community/code-rlaif-dataset
  save_datasets_locally: true
  dataset_output_dir: ./datasets
