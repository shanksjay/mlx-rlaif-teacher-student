# Training Configuration for Code Fine-tuning with RLAIF

# Model Configuration
model:
  base_model: "Qwen/Qwen2.5-Coder-3B-Instruct"  # Baseline Qwen model (smaller, faster for M5 MacBook)
  use_4bit: true  # Use 4-bit quantization for M5 MacBook
  use_flash_attention: true
  max_length: 512  # Reduced from 1024 to prevent MPS OOM (can increase if memory allows)
  device_map: "auto"
  use_safetensors: true  # Use safetensors for faster loading
  low_cpu_mem_usage: true  # Optimize memory during loading

# Teacher Model Configuration
teacher:
  provider: "anthropic"  # Options: "openai" or "anthropic"
  model_name: "claude-3-5-haiku-20241022"  # For OpenAI: "gpt-4-turbo-preview", "gpt-3.5-turbo"
  # For Anthropic: Recommended models (will auto-fallback if unavailable):
  #   - "claude-3-5-haiku-20241022" (fastest, cheapest, recommended)
  #   - "claude-3-5-sonnet-20241022" (better quality, slower, more expensive)
  #   - "claude-3-opus-20240229" (deprecated Jan 2026, but may still work)
  #   - "claude-3-sonnet-20240229" (deprecated Jul 2025, but may still work)
  # Note: The system will automatically try fallback models if your requested model is unavailable.
  # Check available models at: https://console.anthropic.com/settings/keys
  api_key_env: "ANTHROPIC_API_KEY"  # or "OPENAI_API_KEY"
  temperature: 0.7
  max_tokens: 512

# Training Configuration
training:
  output_dir: "./checkpoints"
  num_epochs: 7  # Increased from 3 to 7 for better convergence (5-10 range recommended)
  batch_size: 2  # Reduced from 4 to prevent MPS OOM on 32GB systems
  gradient_accumulation_steps: 32  # Increased from 16 to maintain effective batch size of 64
  learning_rate: 1e-5  # Optimized: reduced from 2e-5 to 1e-5 (range: 5e-6 to 2e-5)
  warmup_steps: 100
  save_steps: 500
  eval_steps: 250
  logging_steps: 50
  max_grad_norm: 1.0
  weight_decay: 0.01
  lr_scheduler_type: "cosine"  # Learning rate scheduling enabled for better convergence
  save_total_limit: 3

# RLAIF Configuration
rlaif:
  reward_weight: 1.0
  kl_penalty: 0.05  # Reduced from 0.1 to allow more exploration (was too conservative)
  beta: 0.1  # Temperature for reward scaling
  top_k: 50
  top_p: 0.95
  num_samples_per_prompt: 6  # Increased from 2 to 6 for more diverse exploration (range: 4-8)
  generation_temperature: 1.0  # Increased from 0.9 to 1.0 for more diversity (range: 0.7-1.2)
  curriculum_learning: true  # Enable curriculum learning (start easy, increase difficulty)
  reward_bonuses: true  # Enable reward bonuses for specific improvements
  use_lora: false  # Use LoRA for efficient fine-tuning (reduces memory, faster training)
  use_qlora: false  # Use QLoRA (4-bit + LoRA) for maximum efficiency (best for memory-constrained systems)
  lora_r: 16  # LoRA rank (higher = more parameters, better quality but slower)
  lora_alpha: 32  # LoRA alpha (scaling factor, typically 2x rank)
  lora_dropout: 0.05  # LoRA dropout rate
  # lora_target_modules: null  # Auto-detect target modules (or specify: ["q_proj", "v_proj", "k_proj", "o_proj"])

# Data Configuration
data:
  train_file: "./data/train.jsonl"
  eval_file: "./data/eval.jsonl"
  languages: ["python", "cpp", "rust"]
  max_train_samples: null  # null means use all
  max_eval_samples: 100

# Evaluation Configuration
evaluation:
  metrics:
    - "code_quality"
    - "correctness"
    - "readability"
    - "efficiency"
  reward_threshold: 0.7  # Minimum reward score to consider good

# Logging Configuration
logging:
  use_tensorboard: true
  tensorboard_dir: "./logs/tensorboard"
  use_wandb: false
  wandb_project: "code-rlaif"
  log_level: "INFO"

# Hardware Configuration (M5 MacBook)
hardware:
  use_mps: true  # Use Metal Performance Shaders for M5
  mixed_precision: "bf16"  # bfloat16 for M5
  dataloader_num_workers: 4
  save_mlx_format: true  # Save model in MLX format for faster inference on Apple Silicon
  mlx_quantization: q8_bit  # Options: "q4_bit", "q8_bit", or null for no quantization
  use_mlx_for_generation: true  # Use MLX for generation (5-10x faster than PyTorch MPS) - enabled by default
  # Auto-detects MLX model if mlx_model_path is null by checking common locations
  mlx_model_path: null  # Path to MLX model (e.g., "./mlx_model/q8", "./mlx_model/q4", "./mlx_model/base")
                         # If null, will auto-detect from: ./mlx_model/q8, ./mlx_model/q4, ./mlx_model/base

  # NVIDIA / CUDA Configuration (for comparison runs on a Linux/Windows box w/ NVIDIA GPU)
  # Unsloth only works on CUDA. On Apple Silicon it will be ignored automatically.
  use_unsloth: false  # Enable Unsloth-optimized loading/training/generation on CUDA
  unsloth_dtype: "bf16"  # "bf16" (recommended on newer GPUs) or "fp16"
  unsloth_max_seq_length: null  # If null, uses model.max_length

# Hugging Face Upload Configuration
huggingface:
  upload_to_hub: false  # Set to true to upload model to Hugging Face
  repo_id: "mlx-community/code-rlaif"  # Format: mlx-community/model-name
  hf_token_env: "HUGGINGFACE_TOKEN"  # Environment variable with HF token
  upload_quantized: true  # Upload quantized version (4-bit) to HF
  private: false  # Make repository private
  upload_datasets: true  # Upload training/validation/evaluation datasets
  dataset_repo_id: "mlx-community/code-rlaif-dataset"  # Dataset repository ID
  save_datasets_locally: true  # Save datasets locally before upload
  dataset_output_dir: "./datasets"  # Local directory to save datasets

